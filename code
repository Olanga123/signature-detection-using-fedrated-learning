import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 1. Generate synthetic 2D Ising data
def generate_ising_data(L=10, num_samples=5000, critical_temp=2.269):
    temps = np.random.uniform(1.0, 3.5, num_samples)
    spins = np.random.choice([-1, 1], size=(num_samples, L, L))
    labels = (temps >= critical_temp).astype(int)  # 0 = ordered, 1 = disordered
    return spins, labels, temps

# 2. Load data
L = 10  # Lattice size
num_samples = 5000
X, y, temperatures = generate_ising_data(L=L, num_samples=num_samples)

# Flatten the 2D lattice into 1D vector
X = X.reshape(num_samples, -1)

# Split into train and test sets
X_train, X_test, y_train, y_test, T_train, T_test = train_test_split(X, y, temperatures, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Needed for logistic regression
y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

# Create DataLoader for mini-batch gradient descent
batch_size = 64
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# 3. Define Logistic Regression Model
class LogisticRegression(nn.Module):
    def _init_(self, input_size):
        super(LogisticRegression, self)._init_()
        self.fc1 = nn.Linear(input_size, 1)  # Single output for binary classification
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        return self.sigmoid(self.fc1(x))

model = LogisticRegression(input_size=L*L)

# 4. Loss function and optimizer
criterion = nn.BCELoss()  # Binary Cross Entropy loss for logistic regression
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 5. Train the model & track loss
num_epochs = 20
losses = []
accuracies = []

for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    losses.append(loss.item())

    with torch.no_grad():
        outputs = model(X_test)
        predicted = (outputs >= 0.5).float()  # Logistic regression thresholding
        accuracy = (predicted == y_test).sum().item() / y_test.size(0)
        accuracies.append(accuracy)

    if (epoch+1) % 5 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy*100:.2f}%')

# 6. Generate Phase Transition Plot
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs+1), accuracies, marker='o', linestyle='-', label='Test Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Logistic Regression Phase Transition Analysis")
plt.legend()
plt.show()

# 7. Magnetization vs. Temperature Plot
plt.figure(figsize=(10, 5))
plt.scatter(T_train, y_train, c=y_train, cmap='coolwarm', alpha=0.5)
plt.xlabel("Temperature")
plt.ylabel("Phase (Ordered=0, Disordered=1)")
plt.title("Magnetization vs. Temperature")
plt.show()

# 8. Visualize Ordered vs. Disordered Spin States
plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.imshow(X[np.argmin(T_train)].reshape(L, L), cmap='gray')
plt.title("Ordered (Low T)")

plt.subplot(1, 2, 2)
plt.imshow(X[np.argmax(T_train)].reshape(L, L), cmap='gray')
plt.title("Disordered (High T)")
plt.show()

print(f'Final Accuracy on Test Set: {accuracies[-1]*100:.2f}%')     project is signature detection using fedrated learning]
